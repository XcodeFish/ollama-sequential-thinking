# Ollama编辑器插件开发指南：核心模块开发 (A)

## 1. 核心模块概述

本文档介绍了Ollama编辑器插件的核心模块开发，包括插件入口、API客户端、Sequential-thinking引擎和上下文管理等关键组件的实现细节。

### 1.1 模块依赖关系

```
extension.ts
  ├── api/client.ts
  ├── core/engine.ts
  │     └── core/context.ts
  └── ui/panel.ts
       └── ui/views.ts
```

## 2. 插件入口模块

`extension.ts` 是插件的入口点，负责注册命令、初始化各模块以及处理插件生命周期。

```typescript
// src/extension.ts
import * as vscode from 'vscode';
import { OllamaClient } from './api/client';
import { SequentialThinkingEngine } from './core/engine';
import { ResultPanel } from './ui/panel';
import { log } from './utils/logger';

// 检测编辑器类型
const isVSCode = vscode.env.appName.includes('Visual Studio Code');
const isCursor = vscode.env.appName.includes('Cursor');

export function activate(context: vscode.ExtensionContext) {
  log('插件激活', 'info');

  // 初始化API客户端
  const config = vscode.workspace.getConfiguration('ollama-sequential-thinking');
  const apiEndpoint = config.get<string>('apiEndpoint') || 'http://localhost:11434/api';
  const defaultModel = config.get<string>('defaultModel') || 'codellama:7b';

  const ollamaClient = new OllamaClient(apiEndpoint, defaultModel);

  // 初始化思考引擎
  const engine = new SequentialThinkingEngine(ollamaClient);

  // 注册命令
  const askQuestionCommand = vscode.commands.registerCommand(
    'ollama-sequential-thinking.askQuestion',
    async () => {
      try {
        // 获取用户输入
        const question = await vscode.window.showInputBox({
          prompt: '请输入您的问题',
          placeHolder: '例如: 如何实现一个React组件?'
        });

        if (!question) return;

        // 显示进度
        await vscode.window.withProgress({
          location: vscode.ProgressLocation.Notification,
          title: '思考中...',
          cancellable: true
        }, async (progress) => {
          // 获取当前编辑器内容作为上下文
          const editor = vscode.window.activeTextEditor;
          const context = editor ? editor.document.getText() : '';

          // 使用思考引擎处理问题
          const result = await engine.process(question, context);

          // 显示结果
          ResultPanel.createOrShow(context.extensionUri, result);
        });
      } catch (error) {
        log(`处理问题时出错: ${error}`, 'error');
        vscode.window.showErrorMessage(`处理失败: ${error}`);
      }
    }
  );

  const generateCodeCommand = vscode.commands.registerCommand(
    'ollama-sequential-thinking.generateCode',
    async () => {
      // 与askQuestion类似，但专注于代码生成
      // ...实现代码生成逻辑
    }
  );

  context.subscriptions.push(askQuestionCommand, generateCodeCommand);

  // 注册状态栏按钮
  const statusBarItem = vscode.window.createStatusBarItem(vscode.StatusBarAlignment.Right, 100);
  statusBarItem.text = '$(brain) Ollama';
  statusBarItem.tooltip = 'Ollama Sequential Thinking';
  statusBarItem.command = 'ollama-sequential-thinking.askQuestion';
  statusBarItem.show();

  context.subscriptions.push(statusBarItem);

  log('插件注册完成', 'info');
}

export function deactivate() {
  log('插件停用', 'info');
}
```

## 3. Ollama API客户端模块

`client.ts` 负责与Ollama服务进行通信，处理API请求和响应。

```typescript
// src/api/client.ts
import axios, { AxiosInstance } from 'axios';
import { log } from '../utils/logger';

export interface OllamaRequestOptions {
  model: string;
  prompt: string;
  stream?: boolean;
  temperature?: number;
  top_p?: number;
  top_k?: number;
  max_tokens?: number;
}

export interface OllamaResponse {
  model: string;
  response: string;
  done: boolean;
  context?: number[];
}

export class OllamaClient {
  private api: AxiosInstance;
  private defaultModel: string;

  constructor(endpoint: string, defaultModel: string) {
    this.api = axios.create({
      baseURL: endpoint,
      headers: {
        'Content-Type': 'application/json'
      }
    });

    this.defaultModel = defaultModel;
    log(`Ollama客户端初始化，端点: ${endpoint}, 默认模型: ${defaultModel}`, 'info');
  }

  async checkConnection(): Promise<boolean> {
    try {
      await this.api.get('/info');
      log('Ollama服务连接成功', 'info');
      return true;
    } catch (error) {
      log(`Ollama服务连接失败: ${error}`, 'error');
      return false;
    }
  }

  async listModels(): Promise<string[]> {
    try {
      const response = await this.api.get('/tags');
      return response.data.models.map((model: any) => model.name);
    } catch (error) {
      log(`获取模型列表失败: ${error}`, 'error');
      return [];
    }
  }

  async generate(options: Partial<OllamaRequestOptions> & { prompt: string }): Promise<string> {
    const requestOptions: OllamaRequestOptions = {
      model: options.model || this.defaultModel,
      prompt: options.prompt,
      stream: options.stream || false,
      temperature: options.temperature || 0.7,
      max_tokens: options.max_tokens || 2048
    };

    try {
      log(`发送请求到Ollama，模型: ${requestOptions.model}`, 'info');

      if (requestOptions.stream) {
        return this.streamGenerate(requestOptions);
      } else {
        const response = await this.api.post('/generate', requestOptions);
        return response.data.response;
      }
    } catch (error) {
      log(`生成请求失败: ${error}`, 'error');
      throw new Error(`Ollama API请求失败: ${error}`);
    }
  }

  private async streamGenerate(options: OllamaRequestOptions): Promise<string> {
    // 流式处理实现
    // 注意：实际实现需要处理流式响应
    return ''; // 占位实现
  }
}
